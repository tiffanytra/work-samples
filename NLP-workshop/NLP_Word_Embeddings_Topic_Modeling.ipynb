{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ac0dbfa",
   "metadata": {},
   "source": [
    "# Word Embeddings: `word2vec`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd699e7e",
   "metadata": {},
   "source": [
    "What are word embeddings? They are a way to transform words into a numerical format, specifically vectors, so they can be used as inputs to a model. Previously, you have seen a way to encode text as numbers using a bag-of-words approach, which takes into account the *frequency* of words to encode the text. However, what if we want to use the *meaning* of the word in our model instead? As humans, when we interpret text, we aren't just looking at how often a word appears, we use the context of the words (which words appear before or after, what words have a similar meaning, etc). <br> <br>\n",
    "Introducing... **word embeddings**!! We embed a word (or a bi-gram, tri-gram, phrase, etc) as a vector in a higher dimensional space. The word embedding model, `word2vec`, is a way to find the vector representations of words (also referred to as tokens). Below illustrates two approaches. <br> <br>\n",
    "The continuous bag-of-words (CBOW) approach predicts a single word using the words that come before and after it. The number of words before and after the target word that is looked at is the called the *window size*. For example, if we had the text `I went to the store to get some apples`, we may try to use the word vectors for `I`, `went`, `to`, `the`, `to`, `get`, `some`, `apples` to predict the word `store`. This would correspond to a *window size* of 4 because there are 4 words on either side of the target word. <br> <br>\n",
    "In the skip-gram model, we input a word and predict what words are related or predict what words we expect would come before and after it. In the above example, we'd aim to predict the remaining words in the sentence from the word vector for `store`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d364790",
   "metadata": {},
   "source": [
    "![word_embedding](data/word_embedding.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9abd42",
   "metadata": {},
   "source": [
    "These models are *neural networks*, a type of a branch of machine learning method called deep learning. The general idea is that there are nodes/neurons that are interconnected with different weights that are learned during the training process. The structure mimics the structure of the human brain, which is why they are called neural networks. Don't worry too much about the details for our purposes today. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ebae1e",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fda7ee",
   "metadata": {},
   "source": [
    "`gensim` is a popular Python package for natural language processing. It is the fastest library for training vector embeddings, can handle large corpus, and has many other NLP uses, such as topic modeling (which we will get into later today!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e30c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below and un if you do not already have gensim installed\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df8f339",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9272fd9d",
   "metadata": {},
   "source": [
    "# Pre-trained Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb0f75b",
   "metadata": {},
   "source": [
    "There are many word embedding models that have already been trained on a large corpus. Thre are many different models trained in different contexts already available on `gensim`. Here are some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef6ea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_models = list(api.info()['models'].keys())\n",
    "print(gensim_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052194d2",
   "metadata": {},
   "source": [
    "We are going to take a closer look at the `word2vec-google-news-300` model: this is a word embedding model that is trained on Google News, where the embedding is 300 dimensions. Downloading this might take a while! The word embedding model is nearly 2 GB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5c8863",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63debc6",
   "metadata": {},
   "source": [
    "There are a variety of methods we can use to explore the embeddings in this model. For example, `.index_to_key` will give us a list of the words/phrases (called the vocabulary) that we have vector embeddings for in this model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 example words in the corpus\n",
    "wv.index_to_key[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d2599",
   "metadata": {},
   "source": [
    "We can find how many word vectors we have in this model by taking the length of this list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9c7da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = len(wv.index_to_key)\n",
    "print(f\"Number of words: {n_words}\")\n",
    "print(wv.index_to_key[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca5443e",
   "metadata": {},
   "source": [
    "The model is trained using a vocabulary of size 3 million! This is a huge model, which takes hours to train. This is why we used a pre-trained model - we likely don't have the resources to train this on our local machines.\n",
    "\n",
    "Accessing the actual word vectors can be done by treating the word vector model as a dictionary. For example, let's take a look at the word vector for `\"banana\"`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64954ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wv[\"banana\"])\n",
    "print(wv[\"banana\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0a3452",
   "metadata": {},
   "source": [
    "The word vector has a dimension of 300. To us, these numbers are pretty uncomprehensible and uninterpretable, but we can now use these vectors to perform \"calculations\" on words to find words that are similar and dissimilar. A particular interesting use of word embeddings is to find words similar by *analogy*. \n",
    "<br> <br>\n",
    "What do we expect below, based on the context we are given? <br> \n",
    "#### **geese - goose + toad = ?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94baf9f8",
   "metadata": {},
   "source": [
    "![word_analogy](data/word_analogy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13b6e9d",
   "metadata": {},
   "source": [
    " If you said toads, that's correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eba1f56",
   "metadata": {},
   "source": [
    "Note if you try to use a word not in the vocabulary (a word that is not present in the corpus the model was trained on), it will result in an error. This is a limitation of `Word2Vec`. It cannot infer vector embeddings for words that it has never seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27af36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will error\n",
    "wv[\"nbvouphr\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f232a55d",
   "metadata": {},
   "source": [
    "# Word Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ff29dc",
   "metadata": {},
   "source": [
    "A semantic question we can ask is  that are similar to \"banana\". How does word similarity look in vector operations? We'd expect similar words to have vectors that are closer to each other in vector space.\n",
    "\n",
    "There are many metrics of vector similarity - one of the most useful ones is the [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity). It has a range of 0 to 1, with orthogonal vectors have a cosine similarity of 0, and parallel vectors having a cosine similarity of 1. `gensim` provides a function that lets us find the most similar vectors to a queried vector - let's give it a shot! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a4ef9",
   "metadata": {},
   "source": [
    "What are some things you notice about the words returned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c7ecf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('watermelon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5ffad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar('happy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4467a9bd",
   "metadata": {},
   "source": [
    "# Distance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743e52ce",
   "metadata": {},
   "source": [
    "The number that appears next to the word is the distance between that word and the word we specified. How \"similar\" a word is to another word is determined by the \"distance\" between the vector embeddings. The higher the distance, the less \"similar\" those words are, since their vector embeddings are far from each. We expect words with similar meaning and contexts to be near each other in vector representation. These patterns are part of what was learned by the neural network during the training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c702af5a",
   "metadata": {},
   "source": [
    "Choose two words and find the distance between their vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67e55e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the blanks with two words, make sure the words are a string\n",
    "wv.distance(..., ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e9427",
   "metadata": {},
   "source": [
    "# Analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ddb26f",
   "metadata": {},
   "source": [
    "We briefly talked about this before, but now let's look at some more examples as well as how to actually perform the calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7aff9b4",
   "metadata": {},
   "source": [
    "`Paris : France :: Berlin : Germany`\n",
    "\n",
    "Here, the analogy is between (Paris, France) and (Berlin, Germany), with \"capital city\" being the concept that connects them. We can abstract the \"analogy\" relationship to vector modeling. Let's pretend we're working with each of the vectors. Then, the analogy is\n",
    "\n",
    "$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} \\approx \\mathbf{v}_{\\text{Germany}} - \\mathbf{v}_{\\text{Berlin}}.$\n",
    "\n",
    "The vector difference here represents the notion of \"capital city\". Presumably, going from the Paris vector to the France vector (i.e., the vector difference) will be the same as going from the Berlin vector to the Germany vector, if that difference carries similar semantic meaning.\n",
    "\n",
    "Let's test this directly. We'll do so by rewriting the above expression:\n",
    "\n",
    "$\\mathbf{v}_{\\text{France}} - \\mathbf{v}_{\\text{Paris}} + \\mathbf{v}_{\\text{Berlin}} \\approx \\mathbf{v}_{\\text{Germany}}.$\n",
    "\n",
    "We'll calculate the difference between Paris and France, add on Germany, and find the closest vector to that quantity. Notice that, in all these operations, we set `norm=True`, and renormalize. That's because different vectors might be of different lengths, so the normalization puts everything on a common scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2883d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate \"capital city\" vector difference\n",
    "difference = wv.get_vector('France', norm=True) - wv.get_vector('Paris', norm=True) \n",
    "# Add on Berlin\n",
    "difference += wv.get_vector('Berlin', norm=True)\n",
    "# Renormalize vector\n",
    "# in linear algebra, the norm of a vector is a way to measure the magnitude of a vector\n",
    "difference /= np.linalg.norm(difference) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9aefdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the most similar vector?\n",
    "wv.most_similar(difference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729a7425",
   "metadata": {},
   "source": [
    "Here is a more concise way to do this same thing using a function we are already familiar with: `most_similar`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0076443f",
   "metadata": {},
   "outputs": [],
   "source": [
    "wv.most_similar(positive=[\"France\", \"Berlin\"], negative=[\"Paris\"])[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55b0680",
   "metadata": {},
   "source": [
    "## Try it out on your own [not covered]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d20031",
   "metadata": {},
   "source": [
    "## Challenge 1\n",
    "\n",
    "Look up the `doesnt_match` function in `gensim`'s documentation. Use this function to identify which word doesn't match in the following group:\n",
    "\n",
    "banana, apple, strawberry, happy\n",
    "\n",
    "Then, try it on groups of words that you choose. Here are some suggestions:\n",
    "\n",
    "1. A group of fruits, and a vegetable. Can it identify that the vegetable doesn't match?\n",
    "2. A group of vehicles that travel by land, and a vehicle that travels by air (e.g., a plane or helicopter). Can it identify the vehicle that flies?\n",
    "3. A group of scientists (e.g., biologist, physicist, chemist, etc.) and a person who does not study an empirical science (e.g., an artist). Can it identify the occupation that is not science based?\n",
    "\n",
    "To be clear, `word2vec` does not learn the precise nature of the differences between these groups. However, the semantic differences correspond to similar words appearing near each other in large corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2e31a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f8858c",
   "metadata": {},
   "source": [
    "## Challenge 2\n",
    "\n",
    "Carry out the following word analogies:\n",
    "\n",
    "1. Mouse : Mice :: Goose : ?\n",
    "2. Kangaroo : Joey :: Cat : ?\n",
    "3. United States : Dollar :: Mexico : ?\n",
    "4. Happy : Sad :: Up : ?\n",
    "5. California : Sacramento :: Canada : ?\n",
    "6. California : Sacramento :: Washington : ?\n",
    "\n",
    "What about something more abstract, such as:\n",
    "\n",
    "7. United States : hamburger :: Canada : ?\n",
    "\n",
    "Some work well, and others don't work as well. Try to come up with your own analogies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6fa8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82df9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9cf2ac",
   "metadata": {},
   "source": [
    "# Training custom word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f24dd22",
   "metadata": {},
   "source": [
    "We've been using pre-trained word embeddings, but you can also train your own word embeddings using a corpus of your choice. Note that training a model on a large corpus will take a long time and be very computationally expensive, so we'll just be using a small corpus today as an example. Generally, larger corpus tend to produce better embeddings, but we can still get meaningful results of a smaller corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199e280f",
   "metadata": {},
   "source": [
    "First, lets load in and preprocess our text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d666a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_path = 'data/airline_tweets.csv'\n",
    "tweets = pd.read_csv(tweets_path, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b06b3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    \"\"\"Preprocesses a string.\"\"\"\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "    # Replace URLs\n",
    "    url_pattern = r'https?:\\/\\/.*[\\r\\n]*'\n",
    "    url_repl = ' URL '\n",
    "    text = re.sub(url_pattern, url_repl, text)\n",
    "    # Replace digits\n",
    "    digit_pattern = '\\d+'\n",
    "    digit_repl = ' DIGIT '\n",
    "    text = re.sub(digit_pattern, digit_repl, text)\n",
    "    # Replace hashtags\n",
    "    hashtag_pattern = r'(?:^|\\s)[＃#]{1}(\\w+)'\n",
    "    hashtag_repl = ' HASHTAG '\n",
    "    text = re.sub(hashtag_pattern, hashtag_repl, text)\n",
    "    # Replace users\n",
    "    user_pattern = r'@(\\w+)'\n",
    "    user_repl = ' USER '\n",
    "    text = re.sub(user_pattern, user_repl, text)\n",
    "    # Remove blank spaces\n",
    "    blankspace_pattern = r'\\s+'\n",
    "    blankspace_repl = ' '\n",
    "    text = re.sub(blankspace_pattern, blankspace_repl, text).strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce0538c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets['text_processed'] = tweets['text'].apply(lambda x: preprocess(x))\n",
    "tweets['text_processed'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b902e",
   "metadata": {},
   "source": [
    "The `Word2Vec` module will allow us to create our own model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d2b1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ca97d5",
   "metadata": {},
   "source": [
    "This model takes in `sentences`, which is a list of lists: the outer list enumerates the documents, and the inner list enumerates the tokens within in each list. So, we need to run a word tokenizer on each of the tweets. Let's use `nltk`'s word tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a0ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f5a82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [word_tokenize(tweet) for tweet in tweets['text_processed']]\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1246cfd",
   "metadata": {},
   "source": [
    "Now, we train the model. We are going to use CBOW to train the model since it's better suited for smaller datasets. Take note of what other arguments we set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d33a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2Vec(\n",
    "    sentences=sentences,\n",
    "    vector_size=30,\n",
    "    window=5,\n",
    "    min_count=1,\n",
    "    sg=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c89862",
   "metadata": {},
   "source": [
    "The model is now trained! Let's take a look at some word vectors. We can access them using the `wv` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0514d7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1891649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv['worst']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e802d11a",
   "metadata": {},
   "source": [
    "Let's explore the word embeddings our model learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce79a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('worst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b349a037",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.distance('great', 'united')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335c405",
   "metadata": {},
   "source": [
    "## Extra: Sentiment Analysis [not covered]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbe2dd6",
   "metadata": {},
   "source": [
    "In the previous module, we used the airline tweets dataset to perform sentiment classification: we tried to classify the sentiment of a text given the bag-of-words representation. Can we do something similar with a word embedding representation?\n",
    "\n",
    "In the word embedding representation, we have an $N$-dimensional vector for each word in a tweet. How can we come up with a representation for the entire tweet?\n",
    "\n",
    "The simplest approach we could take is to simply average the vectors together to come up with a \"tweet representation\". Let's see how this works for predicting sentiment classification.\n",
    "\n",
    "First, we need to subset the dataset into the tweets which only have positive or negative sentiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705ded97",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_binary = tweets[tweets['airline_sentiment'] != 'neutral']\n",
    "y = tweets_binary['airline_sentiment']\n",
    "print(y.value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a60de9",
   "metadata": {},
   "source": [
    "Now, we need to compute the feature matrix. We will query the word vector in each tweet, and come up with an average for the sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7db205",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_size = 30\n",
    "X = np.zeros((len(y), vector_size))\n",
    "\n",
    "# Enumerate over tweets\n",
    "for idx, tweet in enumerate(tweets_binary['text_processed']):\n",
    "    # Tokenize the current tweet\n",
    "    tokens = word_tokenize(tweet)\n",
    "    n_tokens = len(tokens)\n",
    "    # Enumerate over tokens, obtaining word vectors\n",
    "    for token in tokens:\n",
    "        X[idx] += model.wv.get_vector(token)\n",
    "    # Take the average\n",
    "    X[idx] /= n_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525d1372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89307aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673cf5e1",
   "metadata": {},
   "source": [
    "We will be using logistic regression to classify tweets into positive or negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b354480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dfc2368",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train) \n",
    "#if there is a red warning messsage appears, you can ignore it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a4712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Training accuracy: {lr.score(X_train, y_train)}\")\n",
    "print(f\"Test accuracy: {lr.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6526f64",
   "metadata": {},
   "source": [
    "While this performance is pretty good, its not amazing. Here are some considerations we should keep in mind seeing these results.\n",
    "\n",
    "1. We used a word embedding on a relatively small corpus. A word embedding obtained from a very large corpus would perform better. The tricky part in doing this is that our smaller corpus may have some niche tokens that are not in the larger model, so we'd have to work around that.\n",
    "2. We simply averaged word embeddings across tokens. When doing this, we lose meaning in the ordering of words. Other methods, such as `doc2vec`, have been proposed to address these concerns.\n",
    "3. Word embeddings might be an overly complicated approach for the task at hand. In a tweet aimed at an airline, a person needs to convey their sentiment in only 140 characters. So they are more likely to use relatively simple words that easily convey sentiment, making a bag-of-words a natural approach.\n",
    "\n",
    "It's important to note that we also lose out on the interpretability of the logistic regression model, because the actual dimensions of each word vector do not themselves have any meaning. \n",
    "\n",
    "Moral of the story: word embeddings are great, but always start with the simpler model! This is a good way to baseline other approaches, and it might actually work pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14cb3a49",
   "metadata": {},
   "source": [
    "\\newpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f042a6",
   "metadata": {},
   "source": [
    "# Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e73a7",
   "metadata": {},
   "source": [
    "Topic modeling is an unsupervised machine learning used to identify clusters/groups of similar words within a body of text. While this is not clustering, you can think of it as something similar. Topic modeling is often used to characterize a collection of documents by uncovering the abstract \"topics\". It doesn't categorize documents into clusters, but rather groups together words/phrases that are similar, and we can use those words to determine which documents correspond to which \"topic\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2cf5588",
   "metadata": {},
   "source": [
    "Consider genre classification. Some books may neatly fall into one genre, such as mystery, science fiction, etc. However, other books may be considered as incorporating multiple genres. You might have a fantasy novel which has mystery components to it, or a romance novel set in the future. In these cases, we don't want to cluster the fantasy novel into a \"fantasy\" bucket, and the romance novel in a \"romance\" bucket. We'd instead like to have some measure of assigning various topics, with different magnitudes to documents. This is the goal of topic modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5924493e",
   "metadata": {},
   "source": [
    "There are two common approaches to perform topic modeling: non-negative matrix factorization and latent dirichlet allocation (LDA). We will focus on LDA for today."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0466c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is just a plotting helper function for later\n",
    "def plot_top_words(model, feature_names, n_top_words=10, n_row=2, n_col=5, normalize=False):\n",
    "    \"\"\"Plots the top words from a topic model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : topic model object (e.g., LatentDirichletAllocation, NMF)\n",
    "        The trained topic model. It should have a components_ attribute.\n",
    "    feature_names : array-like of strings\n",
    "        The names of each token, as a list or array.\n",
    "    n_top_words : int\n",
    "        The number of words to plot for each topic.\n",
    "    n_row : int\n",
    "        The number of rows in the plot.\n",
    "    n_col : int\n",
    "        The number of columns in the plot.\n",
    "    normalize : boolean\n",
    "        If True, normalizes the components so that they sum to 1 along samples.\n",
    "    \"\"\"\n",
    "    # Create figure\n",
    "    fig, axes = plt.subplots(n_row, n_col, figsize=(3 * n_col, 5 * n_row), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    components = model.components_\n",
    "    # Normalize components, if necessary\n",
    "    if normalize:\n",
    "        components = components / components.sum(axis=1)[:, np.newaxis]\n",
    "    # Iterate over each topic\n",
    "    for topic_idx, topic in enumerate(components):\n",
    "        # Obtain the top words for each topic\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        # Get the token names\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        # Get their values\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        # Plot the token weights as a bar plot\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 20})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        \n",
    "        # Customize plot\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "\n",
    "    return fig, axes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f69e3a1",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8049283",
   "metadata": {},
   "source": [
    "We will be using a new dataset called the **20 Newsgroups** dataset. You can find the original page for this dataset [here](http://qwone.com/~jason/20Newsgroups/).\n",
    "\n",
    "This dataset is comprised of around 18000 newsgroups posts on 20 topics. The split between the train and test set is based upon a messages posted before and after a specific date. The news groups are as follows, with specific labels indicated:\n",
    "\n",
    "* *Computers*\n",
    "    * comp.graphics\n",
    "    * comp.os.ms-windows.misc\n",
    "    * comp.sys.ibm.pc.hardware\n",
    "    * comp.sys.mac.hardware\n",
    "    * comp.windows.x\n",
    "* *Recreation*\n",
    "    * rec.autos\n",
    "    * rec.motorcycles\n",
    "    * rec.sport.baseball\n",
    "    * rec.sport.hockey\n",
    "* *Science*\n",
    "    * sci.crypt\n",
    "    * sci.electronics\n",
    "    * sci.med\n",
    "    * sci.space\n",
    "* *Miscellaneous*\n",
    "    * misc.forsale\n",
    "* *Politics*\n",
    "    * talk.politics.misc\n",
    "    * talk.politics.guns\n",
    "    * talk.politics.mideast\n",
    "* *Religion*\n",
    "    * talk.religion.misc\n",
    "    * alt.atheism\n",
    "    * soc.religion.christian\n",
    "    \n",
    "Let's begin by importing the dataset. We'll use `scikit-learn` to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9eb0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Import fetcher function\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ce8a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always check the documentation! \n",
    "# Note this may take a while to load\n",
    "full_data, labels = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    shuffle=False,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    return_X_y=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3418f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see some data samples\n",
    "print(full_data[5])\n",
    "print('\\n\\n--------\\n\\n')\n",
    "print(full_data[50])\n",
    "print('\\n\\n--------\\n\\n')\n",
    "print(full_data[1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7e13c2",
   "metadata": {},
   "source": [
    "If we take a look at the labels, we see that they're integers, each specifying one of the 20 possible classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055caa36",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(labels))\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79cf7bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    shuffle=True,\n",
    "    random_state=1,\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3426452",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(newsgroups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a759b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca6ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a subset of the data to simplify analysis and save time\n",
    "# feel free experiment with the entire dataset later if you would like\n",
    "n_subsamples = 2000\n",
    "data = full_data[:n_subsamples]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3fb715",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6234d73e",
   "metadata": {},
   "source": [
    "Latent Dirichlet Allocation (LDA) is a Bayesian model that captures how specific topics can generate documents. It is one of the oldest models applied to perform topic modeling.\n",
    "\n",
    "One significant difference between LDA and NMF is that LDA is a *generative* model. This means that it can be used to *generate* new documents, by sampling from it. Assume we have a number of topics $T$. Then, we generate a new document as follows:\n",
    "\n",
    "1. Choose a number of words $N$ according to a Poisson distribution. If you're not familiar with a Poisson distribution, don't worry - the only thing you need to know is that the outputs from a Poisson distribution can only be nonnegative integers (e.g., 0, 1, 2, 3 ...).\n",
    "2. Choose a vector of values $\\boldsymbol{\\theta}=(\\theta_1, \\theta_2, \\ldots, \\theta_T)$ from a Dirichlet distribution. The details of a Dirichlet distribution aren't too important other than that it guarantees all of the $\\theta_i$ add up to 1, and are positive. So, we can think of the $\\theta_i$ as proportions, or probabilities.\n",
    "3. For each of the $N$ words $w_n$:\n",
    "- Choose a topic $t_n$ according to a Multinomial distribution following $\\boldsymbol{\\theta}$. In other words, choose a topic according to the probabilities set by $\\boldsymbol{\\theta}$ (remember, we're thinking of these values as proportions, or probabilities).\n",
    "- Choose a word $w_n$ from a probability distribution $p(w_n|t_n)$ conditioned on $t_n$. This probability distribution is another Multinomial distribution.\n",
    "\n",
    "LDA does not model the order of the words, so in the end, it produces a collection of words - just like the bag of words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4231c27",
   "metadata": {},
   "source": [
    "![lda](data/lda.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ed4cb",
   "metadata": {},
   "source": [
    "There's a lot of variables there, so let's consider a concrete example. Let's suppose we have two topics: soccer and basketball. These are $t_1$ and $t_2$. \n",
    "\n",
    "Some topics are more likely to contains words than others. For example, soccer is more likely to contain `liverpool` and `freekick`, but probably not `nba`. Basketball meanwhile will very likely contain `rebound` and `nba`. Furthermore, even though it's unlikely, a soccer topic might still refer to the `nba`. This unlikeliness is captured through the probabilities assigned in the distribution $p(w_n|t_n)$.\n",
    "\n",
    "Next, each document might consist of multiple \"proportions\" of topics. We've already seen this in NMF, only this time, LDA captures this via a probability distribution rather than a matrix operation. So, Document 1 might mainly be about Soccer, and not really reference basketball - this would be reflected in the probabilities $\\boldsymbol{\\theta}=(0.9, 0.1)$. Meanwhile, another document might equally reference soccer and basketball, so we'd need a different set of parameters $\\boldsymbol{\\theta}=(0.5, 0.5)$.\n",
    "\n",
    "Once again, we're going to use `scikit-learn` to perform LDA. This time, however, we'll use a `CountVectorizer`, since LDA explicitly models *counts*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cb8fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a CountVectorizer\n",
    "n_tokens = 1000\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.95,\n",
    "    min_df=2,\n",
    "    max_features=n_tokens,\n",
    "    stop_words=\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4109349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform CountVectorizer\n",
    "counts = count_vectorizer.fit_transform(data)\n",
    "print(counts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8892e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = count_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2d1c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8330e2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 10\n",
    "random_state = 0\n",
    "\n",
    "lda = LatentDirichletAllocation(\n",
    "    n_components=n_components,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\", # Use when dataset is large\n",
    "    learning_offset=50.0, \n",
    "    random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc05930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the LDA model\n",
    "lda.fit(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805854fe",
   "metadata": {},
   "source": [
    "How can we analyze the trained model? The `lda` object also comes with a `components_` variable, which corresponds to the topic word distribution. Let's plot these values using the function we created above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc81eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "lda.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01bae500",
   "metadata": {},
   "source": [
    "Can you match up the target names with any of these plots below?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9421c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time, we're normalizing - what does this do?\n",
    "fig, axes = plot_top_words(lda, tokens, normalize=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "badafc9f",
   "metadata": {},
   "source": [
    "## Extra: Dimensionality Reduction [not covered]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121149f7",
   "metadata": {},
   "source": [
    "In both NMF and LDA, we broke down the documents into topics. This was, in effect, a *change in representation*. We went from a DTM representation, to a representation of *topics*. \n",
    "\n",
    "Because there are fewer topics than there are tokens, we can think of this as a *dimensionality reduction*. This is desirable for several reasons, the main one being that it's easier to interpret, say, 10 dimensions than it is to interpret 1000.\n",
    "\n",
    "This is computationally true, as well: once we get to higher dimensions, it's harder to compare different vectors with each other, because they generally end up all close to orthogonal. This is known as the *curse of dimensionality*.\n",
    "\n",
    "Let's first transform the counts into the topic representation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436394b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_representation = lda.transform(counts)\n",
    "topic_representation.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11288c1",
   "metadata": {},
   "source": [
    "Let's use a familiar similarity measure to calculate the similarity between pairs of documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687d1e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarily of the first few docs\n",
    "#Notice something special about the diagonal? What does it mean?\n",
    "cosine_similarity(topic_representation[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df887a2c",
   "metadata": {},
   "source": [
    "### Try it yourself: Finding Similar Documents\n",
    "Calculate the cosine similarity between all pairs of documents, and find the two documents whose cosine similarity is the highest. What are these documents? Do they seem similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3db3a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
